"""
Demo script showcasing the new Smart Duplicate Detection feature
"""

import asyncio
import os
import time
from services.manga_crawler import MangaCrawlerService
from services.existence_checker import ExistenceChecker


async def demo_smart_detection():
    """Demonstrate the smart duplicate detection feature"""
    print("ğŸš€ Smart Duplicate Detection Demo")
    print("=" * 50)

    # Test manga URL (you can change this to any manga you want to test)
    test_manga_url = "https://nettruyenvia.com/truyen-tranh/hoc-cung-em-gai-khong-can-thanh-tro-thanh-vo-dich"

    print(f"ğŸ“š Test Manga: {test_manga_url}")
    print(f"ğŸ’¾ Storage Type: Local (you can change to 'cloud' for cloud storage)")

    # Initialize services
    async with MangaCrawlerService() as crawler:
        print(f"\nâœ… MangaCrawlerService initialized")
        print(f"âœ… ExistenceChecker initialized")

        # Get manga info first
        print(f"\nğŸ“– Getting manga information...")
        try:
            async with crawler.session.get(test_manga_url) as response:
                if response.status == 200:
                    html_content = await response.text()
                    manga_title = crawler._extract_manga_title(html_content)
                    print(f"   ğŸ“š Manga Title: {manga_title}")

                    # Get chapter list
                    chapters = await crawler._get_chapter_list(test_manga_url)
                    print(f"   ğŸ“‹ Total Chapters Found: {len(chapters)}")

                    if chapters:
                        print(f"   ğŸ“– Sample Chapters:")
                        for i, (num, title, url) in enumerate(chapters[:5], 1):
                            print(f"      {i}. Chapter {num}: {title}")
                        if len(chapters) > 5:
                            print(f"      ... and {len(chapters) - 5} more")
                else:
                    print(f"   âŒ Failed to access manga page: HTTP {response.status}")
                    return
        except Exception as e:
            print(f"   âŒ Error getting manga info: {str(e)}")
            return

        # Create manga folder path
        sanitized_title = crawler._sanitize_folder_name(manga_title)
        manga_folder = os.path.join('downloads', sanitized_title)

        print(f"\nğŸ“ Manga Folder: {manga_folder}")

        # Check initial progress
        print(f"\nğŸ” Checking initial progress...")
        initial_progress = await crawler.existence_checker.get_manga_progress(manga_folder, "local")
        print(f"   ğŸ“Š Initial Progress: {initial_progress}")

        # Demo 1: First run (download first 3 chapters)
        print(f"\nğŸ¯ Demo 1: First Run - Downloading first 3 chapters")
        print(f"   This will download chapters 1-3")

        start_time = time.time()
        try:
            status, title, folder, total_chapters, chapters_info, errors, processing_time = await crawler.crawl_manga(
                manga_url=test_manga_url,
                max_chapters=3,
                start_chapter=1,
                end_chapter=3,
                delay_between_chapters=1.0,
                image_type="local"
            )

            first_run_time = time.time() - start_time
            print(f"   âœ… First run completed in {first_run_time:.2f}s")
            print(f"   ğŸ“Š Status: {status}")
            print(f"   ğŸ“š Chapters downloaded: {len(chapters_info)}")
            print(f"   ğŸ–¼ï¸ Total images: {sum(len(ch.images) for ch in chapters_info)}")

        except Exception as e:
            print(f"   âŒ First run failed: {str(e)}")
            return

        # Check progress after first run
        print(f"\nğŸ” Checking progress after first run...")
        progress_after_first = await crawler.existence_checker.get_manga_progress(manga_folder, "local")
        print(f"   ğŸ“Š Progress: {progress_after_first}")

        # Demo 2: Second run (should skip existing chapters)
        print(f"\nğŸ¯ Demo 2: Second Run - Should skip existing chapters")
        print(f"   This should be much faster as it skips existing content")

        start_time = time.time()
        try:
            status, title, folder, total_chapters, chapters_info, errors, processing_time = await crawler.crawl_manga(
                manga_url=test_manga_url,
                max_chapters=3,
                start_chapter=1,
                end_chapter=3,
                delay_between_chapters=1.0,
                image_type="local"
            )

            second_run_time = time.time() - start_time
            print(f"   âœ… Second run completed in {second_run_time:.2f}s")
            print(f"   ğŸ“Š Status: {status}")
            print(f"   ğŸ“š Chapters processed: {len(chapters_info)}")
            print(f"   ğŸ–¼ï¸ Total images: {sum(len(ch.images) for ch in chapters_info)}")

            # Show time difference
            time_diff = first_run_time - second_run_time
            if time_diff > 0:
                print(f"   âš¡ Time saved: {time_diff:.2f}s ({((time_diff/first_run_time)*100):.1f}% faster)")
            else:
                print(f"   âš ï¸ No time saved (this might happen if all content was already cached)")

        except Exception as e:
            print(f"   âŒ Second run failed: {str(e)}")
            return

        # Demo 3: Check individual chapter existence
        print(f"\nğŸ” Demo 3: Checking individual chapter existence...")
        for chapter_num in ["1", "2", "3"]:
            exists, images = await crawler.existence_checker.check_chapter_exists(manga_folder, chapter_num, "local")
            print(f"   ğŸ“– Chapter {chapter_num}: exists={exists}, images={len(images)}")

        # Demo 4: Check individual image existence
        print(f"\nğŸ” Demo 4: Checking individual image existence...")
        chapter_1_folder = os.path.join(manga_folder, "Chapter_1")
        if os.path.exists(chapter_1_folder):
            for i in range(1, 4):
                filename = f"{i:03d}.jpg"
                exists = await crawler.existence_checker.check_image_exists(manga_folder, "1", filename, "local")
                print(f"   ğŸ–¼ï¸ Chapter 1/{filename}: exists={exists}")

        # Demo 5: Resume capability (download more chapters)
        print(f"\nğŸ¯ Demo 5: Resume Capability - Download more chapters")
        print(f"   This will download chapters 4-6, skipping 1-3")

        start_time = time.time()
        try:
            status, title, folder, total_chapters, chapters_info, errors, processing_time = await crawler.crawl_manga(
                manga_url=test_manga_url,
                max_chapters=3,
                start_chapter=4,
                end_chapter=6,
                delay_between_chapters=1.0,
                image_type="local"
            )

            resume_time = time.time() - start_time
            print(f"   âœ… Resume run completed in {resume_time:.2f}s")
            print(f"   ğŸ“Š Status: {status}")
            print(f"   ğŸ“š Chapters downloaded: {len(chapters_info)}")
            print(f"   ğŸ–¼ï¸ Total images: {sum(len(ch.images) for ch in chapters_info)}")

        except Exception as e:
            print(f"   âŒ Resume run failed: {str(e)}")

        # Final progress check
        print(f"\nğŸ” Final progress check...")
        final_progress = await crawler.existence_checker.get_manga_progress(manga_folder, "local")
        print(f"   ğŸ“Š Final Progress: {final_progress}")

        print(f"\nğŸ‰ Demo completed!")
        print(f"ğŸ“ Check the downloads folder to see the organized structure:")
        print(f"   {manga_folder}")
        print(f"ğŸ“„ Check the metadata file for progress tracking:")
        print(f"   {os.path.join(manga_folder, 'manga_metadata.json')}")


async def demo_cloud_storage():
    """Demonstrate cloud storage with smart detection"""
    print(f"\nâ˜ï¸ Cloud Storage Demo")
    print("=" * 30)
    print(f"Note: This demo requires proper Wasabi S3 configuration")
    print(f"      Set your AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, etc.")

    try:
        from services.wasabi_service import WasabiService
        wasabi = WasabiService()
        print(f"âœ… Wasabi service initialized")

        # Test cloud storage existence checking
        test_folder = "test_cloud_manga"
        checker = ExistenceChecker()
        await checker.initialize_wasabi()

        print(f"ğŸ” Testing cloud storage existence checking...")
        exists, images = await checker.check_chapter_exists(test_folder, "1", "cloud")
        print(f"   Cloud chapter exists: {exists}")
        print(f"   Cloud images: {images}")

    except Exception as e:
        print(f"âŒ Cloud storage demo failed: {str(e)}")
        print(f"   Make sure you have proper AWS credentials configured")


if __name__ == "__main__":
    print("ğŸš€ Starting Smart Duplicate Detection Demo...")
    print("This demo will show how the system automatically skips existing content")
    print("Make sure you have a stable internet connection for manga crawling")

    # Run the main demo
    asyncio.run(demo_smart_detection())

    # Run cloud storage demo (optional)
    asyncio.run(demo_cloud_storage())

    print(f"\nğŸ¯ Key Benefits Demonstrated:")
    print(f"   âœ… Automatic chapter skipping")
    print(f"   âœ… Individual image detection")
    print(f"   âœ… Progress tracking with metadata")
    print(f"   âœ… Resume capability")
    print(f"   âœ… Time and bandwidth savings")
    print(f"   âœ… Both local and cloud storage support")
