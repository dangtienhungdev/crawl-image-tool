# ğŸ–¼ï¸ Image Crawler API

A powerful FastAPI service for crawling and downloading images from websites with advanced features including lazy loading support, JavaScript rendering, base64 image extraction, and comprehensive manga management.

## âœ¨ Features

### ğŸ–¼ï¸ Single Page Image Crawling

- ğŸŒ **Standard Image Crawling**: Extract images from HTML `<img>` tags
- ğŸ”„ **Lazy Loading Support**: Handle lazy-loaded images using Selenium WebDriver
- ğŸ“± **JavaScript Rendering**: Process dynamically loaded content
- ğŸ¨ **Base64 Images**: Extract and save base64 encoded images
- ğŸ“‚ **Organized Storage**: Automatically save images in domain-named folders
- â˜ï¸ **Cloud Storage Support**: Upload images to Wasabi S3 cloud storage
- âš¡ **Concurrent Downloads**: Fast parallel image downloading with aiohttp
- ğŸ“Š **Status Tracking**: Monitor crawling progress and status

### ğŸ“š Full Manga Series Crawling

- ğŸ“– **Complete Series Download**: Crawl entire manga series with all chapters
- ğŸ—‚ï¸ **Chapter Organization**: Automatic folder structure (Chapter_1/, Chapter_2/, etc.)
- ğŸ”¢ **Sequential Naming**: Images named as 001.jpg, 002.jpg, etc.
- ğŸ¯ **Chapter Range Control**: Download specific chapter ranges or limits
- â±ï¸ **Rate Limiting**: Configurable delays between chapter downloads
- ğŸ›¡ï¸ **Hotlink Protection Bypass**: Proper Referer headers for blocked images
- â˜ï¸ **Cloud Storage Support**: Upload manga images to Wasabi S3 cloud storage
- â„¹ï¸ **Manga Information**: Preview manga details before downloading
- ğŸš« **Smart Duplicate Detection**: Automatically skip existing chapters and images
- ğŸ“Š **Progress Tracking**: Monitor download progress with metadata tracking
- ğŸ”„ **Resume Support**: Resume interrupted downloads without re-downloading

### ğŸ“‹ Manga List Batch Crawling

- ğŸ“š **Batch Processing**: Crawl entire pages of manga lists (e.g., https://nettruyenvia.com/?page=637)
- ğŸ”„ **Automatic Discovery**: Extract all manga URLs from list pages automatically
- âš™ï¸ **Configurable Limits**: Set limits for manga count and chapters per manga
- ğŸ• **Smart Delays**: Configurable delays between manga and chapter downloads
- ğŸ“Š **Comprehensive Reporting**: Detailed statistics for each manga processed
- â˜ï¸ **Cloud Storage Support**: Upload all manga images to Wasabi S3 cloud storage

### ğŸ”§ Advanced Features

- ğŸ›¡ï¸ **Robust Error Handling**: Comprehensive error reporting and recovery
- ğŸ¯ **Clean Architecture**: Well-structured codebase with separation of concerns
- ğŸš« **Anti-Bot Bypass**: Multiple strategies to handle blocked images
- ğŸ” **Health Monitoring**: Service health checks and status endpoints
- ğŸ“ **Detailed Logging**: Comprehensive logging for debugging and monitoring

### ğŸš« Smart Duplicate Detection

- ğŸ” **Chapter-Level Checking**: Automatically detects existing chapters and skips them
- ğŸ–¼ï¸ **Image-Level Checking**: Checks individual images within chapters to avoid re-downloading
- ğŸ’¾ **Dual Storage Support**: Works with both local storage and cloud storage (Wasabi S3)
- ğŸ“Š **Metadata Tracking**: Maintains `manga_metadata.json` files for progress tracking
- âš¡ **Performance Optimization**: Significantly faster re-runs by skipping existing content
- ğŸ”„ **Resume Capability**: Resume interrupted downloads from where they left off
- ğŸ“ˆ **Progress Monitoring**: Track download progress across multiple sessions

## ğŸ—ï¸ Project Structure

```
crawl-image-tool/
â”œâ”€â”€ .git/                          # Git repository
â”œâ”€â”€ .env                           # Environment variables
â”œâ”€â”€ .env.example                   # Environment variables template
â”œâ”€â”€ .gitignore                     # Git ignore file
â”œâ”€â”€ README.md                      # Project documentation
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ main.py                        # FastAPI application entry point
â”œâ”€â”€ system.md                      # System architecture documentation
â”œâ”€â”€ controllers/                   # Business logic controllers
â”‚   â”œâ”€â”€ __init__.py               # Package initialization
â”‚   â”œâ”€â”€ image_controller.py       # Image controller logic
â”‚   â”œâ”€â”€ manga_controller.py       # Manga controller logic
â”‚   â””â”€â”€ manga_list_controller.py  # Manga list controller
â”œâ”€â”€ services/                      # Core services and utilities
â”‚   â”œâ”€â”€ __init__.py               # Package initialization
â”‚   â”œâ”€â”€ image_crawler.py          # Image crawling service
â”‚   â”œâ”€â”€ manga_crawler.py          # Manga crawling service
â”‚   â”œâ”€â”€ manga_list_crawler.py     # Manga list crawling service
â”‚   â””â”€â”€ wasabi_service.py         # Wasabi storage service
â”œâ”€â”€ routes/                        # FastAPI route definitions
â”‚   â”œâ”€â”€ __init__.py               # Package initialization
â”‚   â”œâ”€â”€ image_routes.py           # Image API endpoints
â”‚   â”œâ”€â”€ manga_routes.py           # Manga API endpoints
â”‚   â””â”€â”€ manga_list_routes.py      # Manga list API endpoints
â”œâ”€â”€ models/                        # Data models and schemas
â”‚   â”œâ”€â”€ __init__.py               # Package initialization
â”‚   â””â”€â”€ schemas.py                # Data schemas and models
â””â”€â”€ downloads/                     # Downloaded images storage (auto-created)
```

## ğŸš« Smart Duplicate Detection

The manga crawler now includes intelligent duplicate detection to avoid re-downloading existing content:

### How It Works

1. **Chapter-Level Detection**: Before processing a chapter, the system checks if it already exists
2. **Image-Level Detection**: For each image, it verifies if the file already exists locally or in cloud storage
3. **Metadata Tracking**: Progress is tracked in `manga_metadata.json` files for each manga
4. **Automatic Skipping**: Existing chapters and images are automatically skipped during re-runs

### Benefits

- âš¡ **Faster Re-runs**: Skip existing content automatically
- ğŸ’¾ **Storage Efficiency**: No duplicate files created
- ğŸ”„ **Resume Support**: Continue interrupted downloads seamlessly
- ğŸ“Š **Progress Tracking**: Monitor download status across sessions
- ğŸŒ **Cloud Compatible**: Works with both local and cloud storage

### Example Usage

```bash
# First run - downloads all chapters
curl -X POST "http://localhost:8000/api/v1/manga/crawl" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://nettruyenvia.com/truyen-tranh/example", "image_type": "local"}'

# Second run - automatically skips existing chapters
curl -X POST "http://localhost:8000/api/v1/manga/crawl" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://nettruyenvia.com/truyen-tranh/example", "image_type": "local"}'

# Check progress
curl "http://localhost:8000/api/v1/manga/progress/example_manga_title?image_type=local"
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- Google Chrome or Chromium browser (for Selenium)
- ChromeDriver (will be auto-installed via webdriver-manager)

### Installation

1. **Clone or download the project**:

   ```bash
   cd crawl-image-tool
   ```

2. **Create a virtual environment** (recommended):

   ```bash
   python -m venv venv

   # On Windows
   venv\Scripts\activate

   # On macOS/Linux
   source venv/bin/activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables** (optional):
   ```bash
   cp .env.example .env
   # Edit .env with your Wasabi S3 credentials if using cloud storage
   ```

### Running the Application

1. **Start the FastAPI server**:

   ```bash
   python main.py
   ```

   Or using uvicorn directly:

   ```bash
   uvicorn main:app --host 0.0.0.0 --port 8000 --reload
   ```

2. **Access the API**:
   - API Server: http://localhost:8000
   - Interactive Documentation: http://localhost:8000/docs
   - Alternative Documentation: http://localhost:8000/redoc

## ğŸ“– API Reference

### Base URL
All API endpoints are prefixed with `/api/v1`

### ğŸ–¼ï¸ Image Crawling Endpoints

#### Crawl Images from a Website

**Endpoint**: `POST /api/v1/crawl`

**Request Body**:

```json
{
    "url": "https://nettruyenvia.com/",
    "max_images": 100,
    "include_base64": true,
    "use_selenium": true,
    "image_type": "local",
    "custom_headers": {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
}
```

**Parameters**:

- `url` (required): Website URL to crawl
- `max_images` (optional, default: 100): Maximum number of images to download
- `include_base64` (optional, default: true): Whether to extract base64 encoded images
- `use_selenium` (optional, default: true): Whether to use Selenium for JavaScript rendering
- `image_type` (optional, default: "local"): Storage type - "local" for local files, "cloud" for Wasabi S3
- `custom_headers` (optional): Custom HTTP headers for requests

**Response**:

```json
{
    "status": "success",
    "url": "https://nettruyenvia.com/",
    "domain": "nettruyenvia.com",
    "folder_path": "downloads/nettruyenvia_com",
    "total_images_found": 25,
    "images_downloaded": 23,
    "images": [
        {
            "original_url": "https://nettruyenvia.com/image1.jpg",
            "local_path": "downloads/nettruyenvia_com/image1.jpg",
            "filename": "image1.jpg",
            "cloud_url": "https://s3.ap-southeast-1.wasabisys.com/web-truyen/images/nettruyenvia_com/image1.jpg",
            "size_bytes": 15420,
            "width": 800,
            "height": 600,
            "format": "jpeg"
        }
    ],
    "errors": [],
    "processing_time_seconds": 12.34
}
```

#### Get Crawling Status

**Endpoint**: `GET /api/v1/status/{url:path}`

**Description**: Get the current status and information for a previously crawled URL

**Response**:
```json
{
    "url": "https://nettruyenvia.com/",
    "status": "completed",
    "last_crawled": "2024-01-15T10:30:00Z",
    "total_images": 25,
    "successful_downloads": 23,
    "failed_downloads": 2
}
```

#### Health Check

**Endpoint**: `GET /api/v1/health`

**Response**:

```json
{
    "status": "healthy",
    "service": "Image Crawler API",
    "version": "1.0.0"
}
```

### ğŸ“š Manga Management Endpoints

#### Crawl Entire Manga Series

**Endpoint**: `POST /api/v1/manga/crawl`

**Request Body**:

```json
{
    "url": "https://nettruyenvia.com/truyen-tranh/hoc-cung-em-gai-khong-can-than-tro-thanh-vo-dich",
    "max_chapters": 10,
    "start_chapter": 1,
    "end_chapter": 10,
    "image_type": "local",
    "delay_between_chapters": 2.0,
    "custom_headers": {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
}
```

**Parameters**:

- `url` (required): URL of the manga series page
- `max_chapters` (optional): Maximum number of chapters to download
- `start_chapter` (optional, default: 1): Chapter to start from
- `end_chapter` (optional): Chapter to end at
- `image_type` (optional, default: "local"): Storage type - "local" for local files, "cloud" for Wasabi S3
- `delay_between_chapters` (optional, default: 2.0): Delay between downloads
- `custom_headers` (optional): Custom HTTP headers

**Response**:

```json
{
    "status": "success",
    "manga_url": "https://nettruyenvia.com/truyen-tranh/...",
    "manga_title": "Há»c CÃ¹ng Em GÃ¡i, KhÃ´ng Cáº§n Tháº­n Trá»Ÿ ThÃ nh VÃ´ Äá»‹ch",
    "folder_path": "downloads/Hoc_Cung_Em_Gai_Khong_Can_Than_Tro_Thanh_Vo_Dich",
    "total_chapters": 20,
    "chapters_downloaded": 10,
    "total_images_downloaded": 150,
    "chapters": [
        {
            "chapter_number": "1",
            "chapter_title": "Chapter 1",
            "images_downloaded": 15,
            "status": "success"
        }
    ],
    "processing_time_seconds": 120.5
}
```

**Example Folder Structure**:

```
downloads/
â””â”€â”€ Hoc_Cung_Em_Gai_Khong_Can_Than_Tro_Thanh_Vo_Dich/
    â”œâ”€â”€ Chapter_1/
    â”‚   â”œâ”€â”€ 001.jpg
    â”‚   â”œâ”€â”€ 002.jpg
    â”‚   â””â”€â”€ 003.jpg
    â”œâ”€â”€ Chapter_2/
    â”‚   â”œâ”€â”€ 001.jpg
    â”‚   â”œâ”€â”€ 002.jpg
    â”‚   â””â”€â”€ 004.jpg
    â””â”€â”€ Chapter_3/
        â””â”€â”€ ...
```

#### Get Manga Information

**Endpoint**: `GET /api/v1/manga/info?url={manga_url}`

**Description**: Preview manga information before downloading

**Response**:

```json
{
    "manga_url": "https://nettruyenvia.com/truyen-tranh/...",
    "manga_title": "Há»c CÃ¹ng Em GÃ¡i, KhÃ´ng Cáº§n Tháº­n Trá»Ÿ ThÃ nh VÃ´ Äá»‹ch",
    "total_chapters": 20,
    "chapters": [
        {
            "chapter_number": "166",
            "chapter_title": "Chapter 166",
            "chapter_url": "https://..."
        }
    ]
}
```

### ğŸ“‹ Manga List Batch Processing Endpoints

#### Crawl Manga List Page

**Endpoint**: `POST /api/v1/manga-list/crawl`

**Request Body**:

```json
{
    "url": "https://nettruyenvia.com/?page=637",
    "max_manga": 5,
    "max_chapters_per_manga": 3,
    "image_type": "cloud",
    "delay_between_manga": 3.0,
    "delay_between_chapters": 2.0,
    "custom_headers": {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
}
```

**Parameters**:

- `url` (required): URL of the manga list page (e.g., https://nettruyenvia.com/?page=637)
- `max_manga` (optional): Maximum number of manga to process (None for all)
- `max_chapters_per_manga` (optional): Maximum chapters per manga (None for all)
- `image_type` (optional, default: "local"): Storage type - "local" for local files, "cloud" for Wasabi S3
- `delay_between_manga` (optional, default: 3.0): Delay between manga downloads (seconds)
- `delay_between_chapters` (optional, default: 2.0): Delay between chapter downloads (seconds)
- `custom_headers` (optional): Custom HTTP headers

**Response**:

```json
{
    "status": "success",
    "list_url": "https://nettruyenvia.com/?page=637",
    "total_manga_found": 20,
    "manga_processed": 5,
    "total_images_downloaded": 150,
    "manga_list": [
        {
            "manga_url": "https://nettruyenvia.com/truyen-tranh/manga-1",
            "manga_title": "Manga Title 1",
            "manga_folder": "downloads/Manga_Title_1",
            "total_chapters": 10,
            "chapters_downloaded": 3,
            "total_images_downloaded": 30,
            "status": "success",
            "processing_time_seconds": 45.2,
            "errors": []
        }
    ],
    "errors": [],
    "processing_time_seconds": 180.5
}
```

**Example Folder Structure**:

```
downloads/
â”œâ”€â”€ Manga_Title_1/
â”‚   â”œâ”€â”€ Chapter_1/
â”‚   â”‚   â”œâ”€â”€ 001.jpg
â”‚   â”‚   â””â”€â”€ 002.jpg
â”‚   â””â”€â”€ Chapter_2/
â”‚       â”œâ”€â”€ 001.jpg
â”‚       â””â”€â”€ 002.jpg
â”œâ”€â”€ Manga_Title_2/
â”‚   â””â”€â”€ Chapter_1/
â”‚       â”œâ”€â”€ 001.jpg
â”‚       â””â”€â”€ 002.jpg
â””â”€â”€ ...
```

#### Manga List Health Check

**Endpoint**: `GET /api/v1/manga-list/health`

**Response**:

```json
{
    "status": "healthy",
    "service": "Manga List Crawler API",
    "version": "1.0.0",
    "endpoints": [
        "POST /api/v1/manga-list/crawl - Crawl manga list page"
    ]
}
```

#### Get Manga List Examples

**Endpoint**: `GET /api/v1/manga-list/examples`

**Response**:

```json
{
    "examples": {
        "basic_local": {
            "description": "Basic manga list crawl with local storage",
            "request": {
                "url": "https://nettruyenvia.com/?page=637",
                "max_manga": 3,
                "max_chapters_per_manga": 2,
                "image_type": "local",
                "delay_between_manga": 3.0,
                "delay_between_chapters": 2.0
            }
        },
        "cloud_storage": {
            "description": "Manga list crawl with cloud storage",
            "request": {
                "url": "https://nettruyenvia.com/?page=637",
                "max_manga": 5,
                "max_chapters_per_manga": 3,
                "image_type": "cloud",
                "delay_between_manga": 3.0,
                "delay_between_chapters": 2.0
            }
        }
    }
}
```

### ğŸ  Root Endpoint

#### API Information

**Endpoint**: `GET /`

**Response**:

```json
{
    "message": "Welcome to Image Crawler API",
    "version": "1.0.0",
    "documentation": "/docs",
    "health_check": "/api/v1/health"
}
```

## ğŸ”§ Advanced Features

### Handling Different Image Types

The crawler can handle various image sources:

1. **Standard Images**: Regular `<img>` tags with `src` attributes
2. **Lazy-Loaded Images**: Images with `data-src`, `data-original`, or `data-lazy` attributes
3. **CSS Background Images**: Images defined in `background-image` CSS properties
4. **Base64 Images**: Inline base64 encoded images in HTML
5. **JavaScript-Rendered Images**: Images loaded dynamically via JavaScript

### Selenium Integration

When `use_selenium: true` is set, the crawler:

- Loads the page in a headless Chrome browser
- Scrolls through the entire page to trigger lazy loading
- Waits for JavaScript to execute and render content
- Extracts images from the final rendered DOM

### Error Handling

The API provides comprehensive error reporting:

- Individual image download failures don't stop the entire process
- Partial success status when some images fail to download
- Detailed error messages for troubleshooting
- Graceful fallbacks for various failure scenarios

## ğŸ› ï¸ Configuration

### Environment Variables

You can configure the application using environment variables:

```bash
export HOST=0.0.0.0          # Server host (default: 0.0.0.0)
export PORT=8000             # Server port (default: 8000)
export RELOAD=true           # Auto-reload on code changes (default: true)
```

### Wasabi S3 Cloud Storage Configuration

To enable cloud storage functionality, configure the following environment variables in your `.env` file:

```bash
# Wasabi S3 Configuration
ACCESS_KEY=your_wasabi_access_key
SECRET_KEY=your_wasabi_secret_key
ENDPOINT_URL=https://s3.ap-southeast-1.wasabisys.com
BUCKET_NAME=your_bucket_name
```

**Note**: The system will automatically fall back to local storage if Wasabi configuration is missing or invalid.

### Chrome/Selenium Configuration

The Selenium WebDriver is configured with optimal settings:

- Headless mode for server environments
- Custom user agent to avoid bot detection
- Optimized window size and timeouts
- Automatic ChromeDriver management

## ğŸ“ File Organization

Downloaded images are organized as follows:

```
downloads/
â”œâ”€â”€ nettruyenvia_com/        # Domain-based folder
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.png
â”‚   â””â”€â”€ base64_image_1.jpg
â”œâ”€â”€ Manga_Title_1/           # Manga series folder
â”‚   â”œâ”€â”€ Chapter_1/
â”‚   â”‚   â”œâ”€â”€ 001.jpg
â”‚   â”‚   â””â”€â”€ 002.jpg
â”‚   â””â”€â”€ Chapter_2/
â”‚       â”œâ”€â”€ 001.jpg
â”‚       â””â”€â”€ 002.jpg
â””â”€â”€ ...
```

## ğŸ› Troubleshooting

### Common Issues

1. **ChromeDriver not found**:

   - The webdriver-manager package should auto-install ChromeDriver
   - Ensure Google Chrome or Chromium is installed on your system

2. **Permission denied errors**:

   - Check write permissions for the downloads directory
   - Run with appropriate user permissions

3. **Timeout errors**:

   - Some websites may be slow to load
   - The crawler includes reasonable timeouts and retry logic

4. **Anti-bot protection**:
   - Some websites block automated requests
   - The crawler uses realistic user agents and headers
   - Consider adding delays or custom headers if needed

### Debug Mode

For debugging, you can run the server with detailed logging:

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload --log-level debug
```

## ğŸ“ Example Usage

### With cURL

```bash
# Crawl images from a website (local storage)
curl -X POST "http://localhost:8000/api/v1/crawl" \
     -H "Content-Type: application/json" \
     -d '{
       "url": "https://nettruyenvia.com/",
       "max_images": 50,
       "include_base64": true,
       "use_selenium": true,
       "image_type": "local"
     }'

# Crawl images from a website (cloud storage)
curl -X POST "http://localhost:8000/api/v1/crawl" \
     -H "Content-Type: application/json" \
     -d '{
       "url": "https://nettruyenvia.com/",
       "max_images": 50,
       "include_base64": true,
       "use_selenium": true,
       "image_type": "cloud"
     }'

# Health check
curl -X GET "http://localhost:8000/api/v1/health"

# Crawl manga series
curl -X POST "http://localhost:8000/api/v1/manga/crawl" \
     -H "Content-Type: application/json" \
     -d '{
       "url": "https://nettruyenvia.com/truyen-tranh/...",
       "max_chapters": 5,
       "image_type": "local"
     }'

# Get manga information
curl -X GET "http://localhost:8000/api/v1/manga/info?url=https://nettruyenvia.com/truyen-tranh/..."

# Crawl manga list page (batch processing)
curl -X POST "http://localhost:8000/api/v1/manga-list/crawl" \
     -H "Content-Type: application/json" \
     -d '{
       "url": "https://nettruyenvia.com/?page=637",
       "max_manga": 3,
       "max_chapters_per_manga": 2,
       "image_type": "local",
       "delay_between_manga": 3.0,
       "delay_between_chapters": 2.0
     }'
```

### With Python

```python
import requests
import json

# Crawl images (local storage)
response = requests.post('http://localhost:8000/api/v1/crawl',
    json={
        'url': 'https://nettruyenvia.com/',
        'max_images': 50,
        'include_base64': True,
        'use_selenium': True,
        'image_type': 'local'
    }
)

result = response.json()
print(f"Downloaded {result['images_downloaded']} images to {result['folder_path']}")

# Crawl images (cloud storage)
response = requests.post('http://localhost:8000/api/v1/crawl',
    json={
        'url': 'https://nettruyenvia.com/',
        'max_images': 50,
        'include_base64': True,
        'use_selenium': True,
        'image_type': 'cloud'
    }
)

result = response.json()
print(f"Downloaded {result['images_downloaded']} images")
for image in result['images']:
    if image.get('cloud_url'):
        print(f"Cloud URL: {image['cloud_url']}")

# Get manga information
response = requests.get('http://localhost:8000/api/v1/manga/info',
    params={'url': 'https://nettruyenvia.com/truyen-tranh/...'}
)
manga_info = response.json()
print(f"Manga: {manga_info['manga_title']} - {manga_info['total_chapters']} chapters")

# Crawl manga series
response = requests.post('http://localhost:8000/api/v1/manga/crawl',
    json={
        'url': 'https://nettruyenvia.com/truyen-tranh/...',
        'max_chapters': 5,
        'image_type': 'local'
    }
)

result = response.json()
print(f"Downloaded {result['total_images_downloaded']} images from {result['chapters_downloaded']} chapters")

# Crawl manga list page (batch processing)
response = requests.post('http://localhost:8000/api/v1/manga-list/crawl',
    json={
        'url': 'https://nettruyenvia.com/?page=637',
        'max_manga': 3,
        'max_chapters_per_manga': 2,
        'image_type': 'local',
        'delay_between_manga': 3.0,
        'delay_between_chapters': 2.0
    }
)

result = response.json()
print(f"Processed {result['manga_processed']} manga")
print(f"Total images: {result['total_images_downloaded']}")
for manga in result['manga_list']:
    print(f"- {manga['manga_title']}: {manga['total_images_downloaded']} images")
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## âš ï¸ Disclaimer

This tool is for educational and legitimate use cases only. Always respect website terms of service and robots.txt files. Be mindful of copyright and intellectual property rights when downloading images.

## ğŸ™‹â€â™‚ï¸ Support

If you encounter any issues or have questions:

1. Check the troubleshooting section above
2. Review the API documentation at `/docs`
3. Create an issue in the project repository
4. Check the system architecture documentation in `system.md`
